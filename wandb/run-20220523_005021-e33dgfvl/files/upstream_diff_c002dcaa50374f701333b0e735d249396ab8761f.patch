diff --git a/BPE/bpe_encoder.py b/BPE/bpe_encoder.py
index 7ca0b2e..0bc8e1e 100644
--- a/BPE/bpe_encoder.py
+++ b/BPE/bpe_encoder.py
@@ -40,7 +40,7 @@ class TextEncoder(object):
     """
 
     def __init__(self, encoder_path='BPE/encoder_bpe_40000.json', bpe_path='BPE/vocab_40000.bpe'):
-        self.nlp = spacy.load('en', disable=['parser', 'tagger', 'ner', 'textcat'])
+        self.nlp = spacy.load('en_core_web_sm', disable=['parser', 'tagger', 'ner', 'textcat'])
         self.encoder = json.load(open(encoder_path))
         self.decoder = {v:k for k,v in self.encoder.items()}
         merges = open(bpe_path, encoding='utf-8').read().split('\n')[1:-1]
diff --git a/boot.sh b/boot.sh
new file mode 100755
index 0000000..a9b8f11
--- /dev/null
+++ b/boot.sh
@@ -0,0 +1 @@
+nohup python train.py --save_path save/model > train.log &
diff --git a/data_loader.py b/data_loader.py
index 37eeeda..c65951e 100644
--- a/data_loader.py
+++ b/data_loader.py
@@ -1,6 +1,11 @@
+from copyreg import pickle
 import torch
 import torch.utils.data as data
 import numpy as np
+from tqdm import tqdm
+import pickle
+import os
+from torch.utils.data.distributed import DistributedSampler
 
 
 class Dataset(data.Dataset):
@@ -23,38 +28,56 @@ class Dataset(data.Dataset):
         item = {}
         item['emotion'] = self.emotions[idx]
 
-        item['context_text'] = self.contexts[idx]  # dialog utterance list [ str1, str2, ... ]
-        encoded_context = self.indexer.encode_text(item['context_text'])  # list [ [wordIdx, ...], ... ]
+        # dialog utterance list [ str1, str2, ... ]
+        item['context_text'] = self.contexts[idx]
+        encoded_context = self.indexer.encode_text(
+            item['context_text'])  # list [ [wordIdx, ...], ... ]
         context = []
         context_state = []
         for i, c in enumerate(encoded_context):
-            context += [self.indexer.SOS_IDX] + c + [self.indexer.EOS_IDX]  # add EOS symbol to every sentence's end
+            # add EOS symbol to every sentence's end
+            context += [self.indexer.SOS_IDX] + c + [self.indexer.EOS_IDX]
             ds = self.indexer.DS_SPEAKER_IDX if i % 2 == 0 else self.indexer.DS_LISTENER_IDX
             context_state += [ds for _ in range(len(c) + 2)]
         item['context'] = context
         item['context_state'] = context_state
 
         item['target_text'] = self.targets[idx]  # (str) response
-        encoded_target = self.indexer.encode_text([item['target_text']])[0]  # list [wordIdx,...]
-        target = [self.indexer.SOS_IDX] + encoded_target + [self.indexer.EOS_IDX]  # add EOS symbol to every sentence's end
-        ds = self.indexer.DS_SPEAKER_IDX if len(encoded_context) % 2 == 0 else self.indexer.DS_LISTENER_IDX
+        encoded_target = self.indexer.encode_text([item['target_text']])[
+            0]  # list [wordIdx,...]
+        target = [self.indexer.SOS_IDX] + encoded_target + \
+            [self.indexer.EOS_IDX]  # add EOS symbol to every sentence's end
+        ds = self.indexer.DS_SPEAKER_IDX if len(
+            encoded_context) % 2 == 0 else self.indexer.DS_LISTENER_IDX
         item['target'] = target
         item['target_state'] = [ds for _ in range(len(target))]
 
         if self.test:
             item['dialog'] = torch.tensor(item['context'], dtype=torch.long)
-            item['dialog_state'] = torch.tensor(item['context_state'], dtype=torch.long)
+            item['dialog_state'] = torch.tensor(
+                item['context_state'], dtype=torch.long)
         else:
-            item['dialog'] = torch.tensor(item['context'] + item['target'], dtype=torch.long)
-            item['dialog_state'] = torch.tensor(item['context_state'] + item['target_state'], dtype=torch.long)
+            item['dialog'] = torch.tensor(
+                item['context'] + item['target'], dtype=torch.long)
+            item['dialog_state'] = torch.tensor(
+                item['context_state'] + item['target_state'], dtype=torch.long)
         return item
 
-    def filter_max_len(self, max_len):
-        size = len(self.contexts)
+    def filter_max_len(self, max_len, option):
+        pickle_file = f'filtered_{option}.pickle'
+
         filtered = []
-        for i in range(size):
-            if self[i]['dialog'].shape[0] <= max_len:
-                filtered.append(i)
+        if os.path.isfile(pickle_file):
+            with open(pickle_file, 'rb') as f:
+                filtered = pickle.load(f)
+        else:
+            size = len(self.contexts)
+            for i in tqdm(range(size)):
+                if self[i]['dialog'].shape[0] <= max_len:
+                    filtered.append(i)
+
+            with open(pickle_file, 'wb') as f:
+                pickle.dump(filtered, f)
         self.contexts = self.contexts[filtered]
         self.targets = self.targets[filtered]
         self.emotions = self.emotions[filtered]
@@ -95,18 +118,39 @@ def collate_fn(data, padding_idx):
     return b
 
 
-def get_data_loader(dataset, batch_size, shuffle=True):
+def get_data_loader(dataset, batch_size,shuffle=True):
     return torch.utils.data.DataLoader(dataset=dataset,
-                                      batch_size=batch_size,
-                                      shuffle=shuffle,
-                                      collate_fn=lambda data: collate_fn(data, dataset.indexer.PAD_IDX))
+                                       batch_size=batch_size,
+                                       shuffle=shuffle,
+                                       collate_fn=lambda data: collate_fn(data, dataset.indexer.PAD_IDX))
 
 
 def load_dataset(dataset, indexer, batch_size, test=False, shuffle=True):
     d = {}
-    d['context'] = np.load('empdial_dataset/sys_dialog_texts.%s.npy' % dataset, allow_pickle=True)
-    d['target'] = np.load('empdial_dataset/sys_target_texts.%s.npy' % dataset, allow_pickle=True)
-    d['emotion'] = np.load('empdial_dataset/sys_emotion_texts.%s.npy' % dataset, allow_pickle=True)
+    d['context'] = np.load(
+        'empdial_dataset/sys_dialog_texts.%s.npy' % dataset, allow_pickle=True)
+    d['target'] = np.load(
+        'empdial_dataset/sys_target_texts.%s.npy' % dataset, allow_pickle=True)
+    d['emotion'] = np.load(
+        'empdial_dataset/sys_emotion_texts.%s.npy' % dataset, allow_pickle=True)
     dataset = Dataset(d, indexer, test=test)
     data_loader = get_data_loader(dataset, batch_size, shuffle)
-    return dataset, data_loader
\ No newline at end of file
+    return dataset, data_loader
+
+
+def load_dataset_ddp(dataset, indexer, batch_size, test=False, shuffle=True):
+    d = {}
+    d['context'] = np.load(
+        'empdial_dataset/sys_dialog_texts.%s.npy' % dataset, allow_pickle=True)
+    d['target'] = np.load(
+        'empdial_dataset/sys_target_texts.%s.npy' % dataset, allow_pickle=True)
+    d['emotion'] = np.load(
+        'empdial_dataset/sys_emotion_texts.%s.npy' % dataset, allow_pickle=True)
+    dataset = Dataset(d, indexer, test=test)
+    sampler=DistributedSampler(dataset)
+    data_loader=torch.utils.data.DataLoader(dataset=dataset,
+                                       sampler=sampler,
+                                       batch_size=batch_size,
+                                       shuffle=shuffle,
+                                       collate_fn=lambda data: collate_fn(data, dataset.indexer.PAD_IDX))
+    return dataset,data_loader
diff --git a/filtered_dev.pickle b/filtered_dev.pickle
new file mode 100644
index 0000000..537cd99
Binary files /dev/null and b/filtered_dev.pickle differ
diff --git a/filtered_train.pickle b/filtered_train.pickle
new file mode 100644
index 0000000..4e129d8
Binary files /dev/null and b/filtered_train.pickle differ
diff --git a/train.py b/train.py
index 5cc716a..1111cfa 100644
--- a/train.py
+++ b/train.py
@@ -1,5 +1,8 @@
 import argparse
+from ast import arg
+from dis import dis
 import random
+from turtle import forward
 import numpy as np
 import torch
 import torch.nn.functional as F
@@ -7,11 +10,93 @@ from tensorboardX import SummaryWriter
 from optimizer import OpenAIAdam
 from configs import DEFAULT_MODEL_CFG, DEFAULT_OPT_CFG
 from model import LMModel, load_openai_pretrained_model
-from data_loader import load_dataset
-from utils import make_infinite, stack_input, make_path, \
-                get_time_str, Logger, delete_file, count_parameters
+from data_loader import load_dataset_ddp
+from utils import dotdict, make_infinite, stack_input, make_path, \
+    get_time_str, Logger, delete_file, count_parameters, get_available_gpu
 from time import time
 from indexer import Indexer
+from torch.utils.data.distributed import DistributedSampler
+from torch.nn.parallel import DistributedDataParallel as DDP
+import torch.distributed as dist
+import os
+from pytorch_lightning import LightningModule, seed_everything, Trainer
+from pytorch_lightning.callbacks import ModelCheckpoint
+from pytorch_lightning.loggers import WandbLogger
+available_devices, devices = get_available_gpu()
+
+os.environ['CUDA_VISIBLE_DEVICES'] = devices
+os.environ["TOKENIZERS_PARALLELISM"] = "false"
+
+class AFModel(LightningModule):
+    def __init__(self, model_name,indexer:Indexer,
+                 cfg:dotdict,
+                 n_iter: int = 888,
+                 hidden_size: int = 768,
+                 embed_size: int = 50007,
+                 weight_decay: float = 0.0,
+                 learning_rate: float = 1e-4,
+                 adam_epsilon: float = 1e-8,
+                 train_batch_size: int = 32,
+                 eval_batch_size: int = 32,
+                 warmup_steps: int = 0,
+                 map_size: int = 512,
+                 num_classes: int = 2,
+                 dropout: float = 0.2,
+                 batch_size: int = 64,
+                 max_length: int = 128) -> None:
+        super().__init__()
+        self.save_hyperparameters()
+        self.indexer=indexer
+        self.n_iter = n_iter
+        self.model = LMModel(cfg, self.indexer.n_vocab, self.indexer.n_special, self.indexer.n_ctx)
+
+    def compute_batch_loss(self, batch):
+        # stack token, dialog states and position encoding
+        X = stack_input(batch['dialog'], [batch['dialog_state']], self.indexer)
+        # X = X.to(device)
+        # compute LM logits and loss
+        lm_logits, _ = self(X)
+        # mask = batch['dialog_mask'].to(device)
+        mask = batch['dialog_mask']
+        # calculate language modelling loss
+        target_shifted = X[:, 1:, 0].contiguous().view(-1)
+        lm_logits_shifted = lm_logits[:, :-1, :]
+        lm_logits_shifted = lm_logits_shifted.contiguous().view(-1,
+                                                                lm_logits.shape[-1])
+        loss = F.cross_entropy(
+            lm_logits_shifted, target_shifted, reduction='none')
+        mask_shifted = mask[:, 1:]
+        loss = torch.sum(loss.view(mask_shifted.shape) *
+                         mask_shifted) / torch.sum(mask_shifted)
+        return loss
+
+    def forward(self, x):
+        return self.model(x)
+
+    def training_step(self, batch, batch_idx):
+        loss = self.compute_batch_loss(batch)
+
+        return loss
+
+    def validation_step(self, batch, batch_idx):
+        val_loss = self.compute_batch_loss(batch)
+        return val_loss
+
+    def configure_optimizers(self):
+
+        model_opt = OpenAIAdam(self.parameters(),
+                               t_total=self.n_iter,
+                               lr=DEFAULT_OPT_CFG.lr,
+                               schedule=DEFAULT_OPT_CFG.lr_schedule,
+                               warmup=DEFAULT_OPT_CFG.lr_warmup,
+                               b1=DEFAULT_OPT_CFG.b1,
+                               b2=DEFAULT_OPT_CFG.b2,
+                               e=DEFAULT_OPT_CFG.e,
+                               l2=DEFAULT_OPT_CFG.l2,
+                               vector_l2=DEFAULT_OPT_CFG.vector_l2,
+                               max_grad_norm=DEFAULT_OPT_CFG.max_grad_norm)
+
+        return model_opt
 
 
 def parse_args():
@@ -27,55 +112,37 @@ def parse_args():
     parser.add_argument('--log_file', type=str, default='train.output')
     parser.add_argument('--save_path', type=str, default='save/best_params')
     parser.add_argument('--print_to', type=str, default='file')
+    parser.add_argument('--dev', type=bool, default=True)
+    parser.add_argument('--test', type=bool, default=False)
     parser.add_argument('--seed', type=int, default=42)
     parser.add_argument('--no_pretrained', default=False, action='store_true')
-    parser.add_argument('--pretrained_dir', type=str, default='save/pretrained_lm/')
+    parser.add_argument('--pretrained_dir', type=str,
+                        default='save/pretrained_lm/')
     return parser.parse_args()
 
 
-def compute_batch_loss(model, batch):
-    # stack token, dialog states and position encoding
-    X = stack_input(batch['dialog'], [batch['dialog_state']], indexer)
-    X = X.to(device)
-    # compute LM logits and loss
-    lm_logits, _ = model(X)
-    mask = batch['dialog_mask'].to(device)
-    # calculate language modelling loss
-    target_shifted = X[:, 1:, 0].contiguous().view(-1)
-    lm_logits_shifted = lm_logits[:, :-1, :]
-    lm_logits_shifted = lm_logits_shifted.contiguous().view(-1, lm_logits.shape[-1])
-    loss = F.cross_entropy(lm_logits_shifted, target_shifted, reduction='none')
-    mask_shifted = mask[:, 1:]
-    loss = torch.sum(loss.view(mask_shifted.shape) * mask_shifted) / torch.sum(mask_shifted)
-    return loss
-
-
-def validate(model, data):
-    val_loss = []
-    with torch.no_grad():
-        model.eval()
-        for _, batch in enumerate(data):
-            l = compute_batch_loss(model, batch)
-            val_loss.append(l.item())
-    return np.mean(val_loss)
-
+# def validate(model, data):
+#     val_loss = []
+#     with torch.no_grad():
+#         model.eval()
+#         for _, batch in enumerate(data):
+#             l = compute_batch_loss(model, batch)
+#             val_loss.append(l.item())
+#     return np.mean(val_loss)
 
 
 if __name__ == '__main__':
 
     args = parse_args()
+
     random.seed(args.seed)
     np.random.seed(args.seed)
     torch.manual_seed(args.seed)
     torch.cuda.manual_seed_all(args.seed)
 
-    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
+    # device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
 
-    # logging
-    logger = Logger(args.print_to, args.log_dir, args.log_file)
-    logger.log(args)
     # training record by tensorboardX
-    tb_writer = SummaryWriter(log_dir=args.log_dir)
 
     # batch size
     batch_size = args.n_batch
@@ -84,102 +151,139 @@ if __name__ == '__main__':
     # indexer
     indexer = Indexer(cfg.n_ctx)
 
+    # set wandb logger
+    wandb_logger = WandbLogger(project='empathetic_dialogue',
+                               config={
+                                   "epochs": args.n_epoch,
+                                   "batch_size": args.n_batch,
+                                   "lr": DEFAULT_OPT_CFG.lr,
+                                   "schedule": DEFAULT_OPT_CFG.lr_schedule,
+                                   "warmup": DEFAULT_OPT_CFG.lr_warmup,
+                                   "b1": DEFAULT_OPT_CFG.b1,
+                                   "b2": DEFAULT_OPT_CFG.b2,
+                                   "e": DEFAULT_OPT_CFG.e,
+                                   "l2": DEFAULT_OPT_CFG.l2,
+                                   "vector_l2": DEFAULT_OPT_CFG.vector_l2,
+                                   "max_grad_norm": DEFAULT_OPT_CFG.max_grad_norm,
+                                   "n_ctx":cfg.n_ctx
+                               })
     # load train, dev data
-    trainset, data_loader_train = load_dataset('train', indexer, batch_size)
-    devset, data_loader_dev = load_dataset('dev', indexer, batch_size)
+    trainset, data_loader_train = load_dataset_ddp(
+        'train', indexer, batch_size)
+    devset, data_loader_dev = load_dataset_ddp('dev', indexer, batch_size)
     # to avoid memory overflow
-    trainset.filter_max_len(indexer.n_ctx)
-    devset.filter_max_len(indexer.n_ctx)
+    trainset.filter_max_len(indexer.n_ctx, 'train')
+    devset.filter_max_len(indexer.n_ctx, 'dev')
+
+
+    tr_iter = make_infinite(data_loader_train)
+    n_epoch = args.n_epoch
+    n_iter = int(np.ceil(len(trainset) / batch_size)) * n_epoch
 
     # create and load pretrained model
-    model = LMModel(cfg, indexer.n_vocab, indexer.n_special, indexer.n_ctx)
+    model = AFModel(model_name='adde-lm',indexer=indexer,cfg=cfg,n_iter=n_iter)
     if not args.no_pretrained:
-        load_openai_pretrained_model(model.transformer, cfg,
+        load_openai_pretrained_model(model.model.transformer, cfg,
                                      n_special=indexer.n_special,
                                      dir=args.pretrained_dir)
 
-    logger.log('Model params: %d' % count_parameters(model))
-    model.to(device)
-
 
     #################### training ####################
-    tr_iter = make_infinite(data_loader_train)
-    n_epoch = args.n_epoch
-    n_iter = int(np.ceil(len(trainset) / batch_size)) * n_epoch
 
+    best_valid_ppl = 1000000
+    best_param_path = make_path(args.save_path)
+    max_patience = args.max_patience
+    patience = max_patience
+    check_iter = args.check_iter
+    print_iter = args.print_iter
+    trainer=Trainer
+    checkpoint_callback = ModelCheckpoint(
+        monitor="val_loss", filename=f"adde_model", mode="min")
+    trainer = Trainer(max_epochs=n_epoch,
+                      accelerator="gpu",
+                      devices=available_devices,
+                      strategy='bagua',
+                      callbacks=[checkpoint_callback]
+                      )
+    if args.is_dev:
+        trainer = Trainer(max_epochs=5,
+                          accelerator="gpu",
+                          devices=1,
+                          )
+    if not args.is_test:
+        trainer.fit(model=model, train_dataloaders=data_loader_train, val_dataloaders=data_loader_dev)
+    
+    if args.is_test:
+        version_num=0
+        checkpoints_file=f'./lightning_logs/version_{version_num}/checkpoints/adde_model.ckpt'
+        model.load_from_checkpoint(checkpoints_file)
+        # trainer.test(model=model, dataloaders=edl)
     # optimizer
-    model_opt = OpenAIAdam(model.parameters(),
-                           t_total=n_iter,
-                           lr=DEFAULT_OPT_CFG.lr,
-                           schedule=DEFAULT_OPT_CFG.lr_schedule,
-                           warmup=DEFAULT_OPT_CFG.lr_warmup,
-                           b1=DEFAULT_OPT_CFG.b1,
-                           b2=DEFAULT_OPT_CFG.b2,
-                           e=DEFAULT_OPT_CFG.e,
-                           l2=DEFAULT_OPT_CFG.l2,
-                           vector_l2=DEFAULT_OPT_CFG.vector_l2,
-                           max_grad_norm=DEFAULT_OPT_CFG.max_grad_norm)
-
-    try:
-        best_valid_ppl = 1000000
-        best_param_path = make_path(args.save_path)
-        max_patience = args.max_patience
-        patience = max_patience
-        check_iter = args.check_iter
-        print_iter = args.print_iter
-
-        logger.log('Begin training.')
-        logger.log('total training data: %d' % len(trainset))
-        logger.log('batch size = %d, epochs: %d, total iterations: %d ' % (batch_size, n_epoch, n_iter))
-        logger.log('-'*89)
-
-        start_time = time()
-        logger.log('Start time: %s' % get_time_str())
-        for i_iter in np.arange(1, n_iter+1):
-            batch = next(tr_iter)
-            model.train()
-            # compute loss
-            loss = compute_batch_loss(model, batch)
-            # update
-            loss.backward()
-            model_opt.step()
-            model_opt.zero_grad()
-            # log
-            perplexity = np.exp(min(loss.item(), 100))
-            tb_writer.add_scalars('loss', {'loss_train': loss.item()}, i_iter)
-            tb_writer.add_scalars('ppl', {'ppl_train': perplexity}, i_iter)
-            if i_iter % print_iter == 0:
-                tmp = i_iter % check_iter
-                tmp = check_iter if tmp == 0 else tmp
-                avg_seconds = (time() - start_time) / tmp
-                logger.log('iter %d, avg time per iter: %f' % (i_iter, avg_seconds))
-            # validate
-            if i_iter % check_iter == 0:
-                logger.log('-'*10+'start validation at iter %d' % i_iter)
-                start_time = time()
-                val_loss = validate(model, data_loader_dev)
-                val_ppl = np.exp(min(val_loss, 100))
-                tb_writer.add_scalars('loss', {'loss_valid': val_loss}, i_iter)
-                tb_writer.add_scalars('ppl', {'ppl_valid': val_ppl}, i_iter)
-                logger.log('loss=%f, ppl=%f' % (val_loss, val_ppl))
-                logger.log('-'*10+'time for validation: %f' % (time()-start_time))
-                start_time = time()
-                if val_ppl < best_valid_ppl:
-                    patience = max_patience
-                    best_valid_ppl = val_ppl
-                    # save params
-                    logger.log('@@ save best params at iter %d, ppl=%.2f' % (i_iter, val_ppl))
-                    delete_file(best_param_path)
-                    torch.save(model.state_dict(), best_param_path)
-                else:
-                    patience -= 1
-                    if patience == 0:
-                        logger.log('-' * 89)
-                        logger.log('Exiting from traning at iter %d' % i_iter)
-                        break  # break training iteration
-    except KeyboardInterrupt:
-        logger.log('-' * 89)
-        logger.log('Exiting from training early')
-
-    logger.log('Training end at: %s' % get_time_str())
-    logger.close()
+
+    # try:
+        # best_valid_ppl = 1000000
+        # best_param_path = make_path(args.save_path)
+        # max_patience = args.max_patience
+        # patience = max_patience
+        # check_iter = args.check_iter
+        # print_iter = args.print_iter
+
+        # logger.log('Begin training.')
+        # logger.log('total training data: %d' % len(trainset))
+        # logger.log('batch size = %d, epochs: %d, total iterations: %d ' %
+        #            (batch_size, n_epoch, n_iter))
+        # logger.log('-'*89)
+
+    #     start_time = time()
+    #     logger.log('Start time: %s' % get_time_str())
+    #     for i_iter in np.arange(1, n_iter+1):
+    #         batch = next(tr_iter)
+    #         model.train()
+    #         # compute loss
+    #         loss = compute_batch_loss(model, batch)
+    #         # update
+    #         loss.backward()
+    #         model_opt.step()
+    #         model_opt.zero_grad()
+    #         # log
+    #         perplexity = np.exp(min(loss.item(), 100))
+    #         tb_writer.add_scalars('loss', {'loss_train': loss.item()}, i_iter)
+    #         tb_writer.add_scalars('ppl', {'ppl_train': perplexity}, i_iter)
+    #         if i_iter % print_iter == 0:
+    #             tmp = i_iter % check_iter
+    #             tmp = check_iter if tmp == 0 else tmp
+    #             avg_seconds = (time() - start_time) / tmp
+    #             logger.log('iter %d, avg time per iter: %f' %
+    #                        (i_iter, avg_seconds))
+    #         # validate
+    #         if i_iter % check_iter == 0:
+    #             logger.log('-'*10+'start validation at iter %d' % i_iter)
+    #             start_time = time()
+    #             val_loss = validate(model, data_loader_dev)
+    #             val_ppl = np.exp(min(val_loss, 100))
+    #             tb_writer.add_scalars('loss', {'loss_valid': val_loss}, i_iter)
+    #             tb_writer.add_scalars('ppl', {'ppl_valid': val_ppl}, i_iter)
+    #             logger.log('loss=%f, ppl=%f' % (val_loss, val_ppl))
+    #             logger.log('-'*10+'time for validation: %f' %
+    #                        (time()-start_time))
+    #             start_time = time()
+    #             if val_ppl < best_valid_ppl:
+    #                 patience = max_patience
+    #                 best_valid_ppl = val_ppl
+    #                 # save params
+    #                 logger.log('@@ save best params at iter %d, ppl=%.2f' %
+    #                            (i_iter, val_ppl))
+    #                 delete_file(best_param_path)
+    #                 torch.save(model.state_dict(), best_param_path)
+    #             else:
+    #                 patience -= 1
+    #                 if patience == 0:
+    #                     logger.log('-' * 89)
+    #                     logger.log('Exiting from traning at iter %d' % i_iter)
+    #                     break  # break training iteration
+    # except KeyboardInterrupt:
+    #     logger.log('-' * 89)
+    #     logger.log('Exiting from training early')
+
+    # logger.log('Training end at: %s' % get_time_str())
+    # logger.close()
diff --git a/utils.py b/utils.py
index bb5bdd1..d199d30 100644
--- a/utils.py
+++ b/utils.py
@@ -6,6 +6,8 @@ import sys
 import re
 import subprocess
 import tempfile
+from typing import Tuple
+import GPUtil
 
 
 ########### common utils ###########
@@ -130,4 +132,23 @@ def moses_multi_bleu(hypotheses, references, lowercase=True):
     # Close temp files
     hypothesis_file.close()
     reference_file.close()
-    return bleu_score
\ No newline at end of file
+    return bleu_score
+
+
+def get_available_gpu() -> Tuple[int, str]:
+    """return a tuple
+
+    Returns
+    -------
+    Tuple[int,str]
+        return tuple
+    """
+    device_ids = GPUtil.getAvailable(
+        order='first',
+        limit=8,
+        maxLoad=0.3,
+        maxMemory=0.3,
+        includeNan=False,
+        excludeID=[],
+        excludeUUID=[])
+    return len(device_ids), ','.join([str(i) for i in device_ids])
